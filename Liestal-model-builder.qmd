---
title: "Liestal Model Builder"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(chillR)
library(randomForest)
library(caret)
library(Matrix)
library(xgboost)
library(zoo)
library(parallel)
```

```{r}
liestal <- read_csv("Data/liestal-weather-data.csv")
head(liestal)
```

```{r}
liestal_bloom <- read.csv("Data/liestal.csv")
str(liestal_bloom)
```

```{r}
liestal_bloom_only <- liestal_bloom %>%
  select(bloom_date,bloom_doy) %>%
  mutate(bloom_date = as.Date(bloom_date))
```

## Basic Temperature variables

```{r}
liestal_seasons <- liestal %>%
  mutate(
    YEAR = year(DATE),
    MONTH = month(DATE),
    SEASON = case_when(
      MONTH %in% c(9, 10, 11) ~ "Fall",
      MONTH %in% c(12, 1, 2) ~ "Winter",
      MONTH %in% c(3, 4, 5) ~ "Spring",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(SEASON))
head(liestal_seasons)
```

```{r}
liestal_temp_weather_summary <- liestal_seasons %>%
  group_by(YEAR, SEASON) %>%
  summarise(
    ACC_TMAX = sum(TMAX, na.rm = TRUE),
    ACC_TMIN = sum(TMIN, na.rm = TRUE),
    ACC_TAVG = sum(TAVG, na.rm = TRUE),
    ACC_PRCP = sum(PRCP, na.rm = TRUE),
    AVG_TMAX = mean(TMAX, na.rm = TRUE),
    AVG_TMIN = mean(TMIN, na.rm = TRUE),
    AVG_TAVG = mean(TAVG, na.rm = TRUE),
    AVG_PRCP = mean(PRCP, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = SEASON,
    values_from = c(ACC_TMAX, ACC_TMIN, ACC_TAVG, ACC_PRCP, AVG_TMAX, AVG_TMIN, AVG_TAVG, AVG_PRCP),
    names_glue = "{.value}_{SEASON}"
  )
head(liestal_temp_weather_summary)
```

## Growing degree days

The first variable to make is cumulative growing degree days (GDD). GDD are a measure of heat accumulation used by gardners and farmers to predict plant and animal development rates such as the date that a flower will bloom. (Source: https://en.wikipedia.org/wiki/Growing_degree-day)


```{r}
temp_data <- liestal %>%
  mutate(
    Year = as.numeric(format(DATE, "%Y")),
    Month = as.numeric(format(DATE, "%m")),
    Day = as.numeric(format(DATE, "%d"))
  ) %>%
  select(Year, Month, Day, Tmin = TMIN, Tmax = TMAX)
hourly_temps <- stack_hourly_temps(temp_data, latitude = 47.4814)$hourtemps
```

```{r}
chilling_start <- "10-01"  
chilling_end <- "01-01"    

# Filter data for the chilling period
chilling_data <- hourly_temps %>%
  filter(
    (paste0(Month, "-", Day) >= chilling_start & Month >= 11) |  
    (paste0(Month, "-", Day) <= chilling_end & Month <= 2)       
  )

chill_results <- chilling(
  hourtemps  = chilling_data,
  Start_JDay = min(chilling_data$JDay),  
  End_JDay = max(chilling_data$JDay)     
)
```

```{r}

T_base <- 4  
T_max_cap <- 36  

liestal_GDD <- liestal %>%
  mutate(
    Year = year(DATE),
    TAVG_adj = pmin(TAVG, T_max_cap),
    GDD = ifelse(DATE >= as.Date(paste0(Year, "-01-01")) & DATE <= as.Date(paste0(Year, "-05-16")), 
                 pmax(TAVG_adj - T_base, 0), 0)  
  )
```

```{r}
liestal_GDD <- liestal_GDD %>%
  group_by(Year) %>%
  mutate(GDD_cumsum = cumsum(GDD)) %>%
  ungroup()
```

```{r}
liestal_GDD <- liestal_GDD %>%
  filter(DATE >= as.Date(paste0(Year, "-01-01")) & DATE <= as.Date(paste0(Year, "-05-16"))) %>%
  mutate(JDay = as.numeric(format(DATE, "%j")) - as.numeric(format(as.Date(paste0(Year, "-01-01")), "%j")) + 1)  

gdd_matrix <- liestal_GDD %>%
  select(Year, JDay, GDD_cumsum) %>%
  pivot_wider(names_from = JDay, values_from = GDD_cumsum, names_prefix = "CUMSUM_GDD_DAY_") %>%
  filter(row_number() <= n()-1) %>%
  mutate(
    Chilling_Hours = chill_results$Chilling_Hours,
    Utah_Model = chill_results$Utah_Model,
    Chill_Portions = chill_results$Chill_portions,
    GDH =chill_results$GDH
  ) %>%
  select(-CUMSUM_GDD_DAY_137)

head(gdd_matrix)
```

GDD and chilling data have been formed ordered by row(each year). Time for he next feature.

## Previous bloom history

```{r}
liestal_bloom_lag <- liestal_bloom_only %>%
  arrange(bloom_date) %>% 
  mutate(
    last_year_bloom_doy = lag(bloom_doy, 1),
    avg_last_5 = rollapply(bloom_doy, width = 5, FUN = mean, align = "right", fill = NA, partial = TRUE),
    sd_last_5 = rollapply(bloom_doy, width = 5, FUN = sd, align = "right", fill = NA, partial = TRUE),
    avg_last_10 = rollapply(bloom_doy, width = 10, FUN = mean, align = "right", fill = NA, partial = TRUE),
    sd_last_10 = rollapply(bloom_doy, width = 10, FUN = sd, align = "right", fill = NA, partial = TRUE),
    Year = year(bloom_date) 
  ) %>%
  select(-bloom_date,-bloom_doy)
head(liestal_bloom_lag)
```

```{r}
merged_data <- left_join(gdd_matrix,liestal_bloom_lag,by=join_by(Year == Year)) %>%
  left_join(.,liestal_temp_weather_summary,by=join_by(Year==YEAR))
bloom_data <- tail(liestal_bloom_only, 124)
```

```{r}
final_data <-cbind(merged_data , bloom_doy = bloom_data$bloom_doy)

set.seed(490)
train_index <- createDataPartition(final_data$bloom_doy, p = 0.8, list = FALSE)
train_data <- final_data[train_index, ]
test_data <- final_data[-train_index, ]
```


```{r}
set.seed(490)
rf_model <- randomForest(bloom_doy ~ .,
                         data = train_data,
                         ntree = 50,
                         mtry=50)

predictions <- predict(rf_model, test_data)

rmse <- sqrt(mean((predictions - test_data$bloom_doy)^2))
mae <- mean(abs(predictions - test_data$bloom_doy))

cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
```
Good mae score, now to do 3-fold CV to identify the best param combo to maximize ae


```{r}
set.seed(490)

ntree_values <- seq(10, 300, by = 10)  
mtry_values <- seq(5, 170, by = 10)    
nodesize_values <- seq(1, 50, by = 5)  

param_grid <- expand.grid(ntree_values, mtry_values, nodesize_values)

num_cores <- detectCores(logical = F) - 1  

train_control <- trainControl(method = "cv", number = 3)

train_rf <- function(params) {
  nt <- params[1]
  mt <- params[2]
  ns <- params[3]
  
  rf_model <- train(
    bloom_doy ~ .,
    data = train_data,
    method = "rf",
    trControl = train_control,
    tuneGrid = data.frame(mtry = mt),
    ntree = nt,
    nodesize = ns
  )
  
  predictions <- predict(rf_model, test_data)
  
  rmse <- sqrt(mean((predictions - test_data$bloom_doy)^2))
  mae <- mean(abs(predictions - test_data$bloom_doy))
  
  return(data.frame(ntree = nt, mtry = mt, nodesize = ns, RMSE = rmse, MAE = mae))
}

# Parallel Processing to speed up 
cl <- makeCluster(num_cores)
clusterExport(cl, varlist = c("train_rf", "train_data", "test_data", "train_control", "param_grid"))
clusterEvalQ(cl, library(randomForest))
clusterEvalQ(cl, library(caret))

results_list <- parLapply(cl, 1:nrow(param_grid), function(i) {
  train_rf(as.numeric(param_grid[i, ]))
})

stopCluster(cl)

results <- bind_rows(results_list)
```
```{r}
#write.csv(results, "Data/liestal_rf_results.csv", row.names = FALSE)
```

Now, time to try xgboost.

```{r}
set.seed(490)

nrounds_values <- c(50,150,200,250) 
max_depth_values <- c(3, 6, 9)
eta_values <- c(0.01, 0.05, 0.1)
min_child_weight_values <- c(1, 3, 5)
subsample_values <- c(0.6, 0.8, 1.0)
colsample_bytree_values <- c(0.6, 0.8, 1.0)
gamma_values <- c(0, 0.1, 1, 5)

param_grid <- expand.grid(nrounds = nrounds_values,
                          max_depth = max_depth_values,
                          eta = eta_values,
                          min_child_weight = min_child_weight_values,
                          subsample = subsample_values,
                          colsample_bytree = colsample_bytree_values,
                          gamma = gamma_values)


num_cores <- detectCores(logical = F) - 1  


train_control <- trainControl(method = "cv", number = 3)


train_xgb <- function(params) {
  nrounds <- params[1]
  max_depth <- params[2]
  eta <- params[3]
  min_child_weight <- params[4]
  subsample <- params[5]
  colsample_bytree <- params[6]
  gamma <- params[7]

   xgb_model <- train(
    bloom_doy ~ .,
    data = train_data,
    method = "xgbTree",
    trControl = train_control,
    tuneGrid = data.frame(nrounds = nrounds, max_depth = max_depth, eta = eta,
                          min_child_weight = min_child_weight, subsample = subsample,
                          colsample_bytree = colsample_bytree, gamma=gamma),
    metric = "MAE",
    objective = "reg:absoluteerror"
  )

  predictions <- predict(xgb_model, test_data)

  rmse <- sqrt(mean((predictions - test_data$bloom_doy)^2))
  mae <- mean(abs(predictions - test_data$bloom_doy))

  return(data.frame(nrounds = nrounds, max_depth = max_depth, eta = eta, gamma=gamma,
                    min_child_weight = min_child_weight, subsample = subsample,
                    colsample_bytree = colsample_bytree, RMSE = rmse, MAE = mae))
}


cl <- makeCluster(num_cores)
clusterExport(cl, varlist = c("train_xgb", "param_grid", "train_data", "test_data", "train_control"))
clusterEvalQ(cl, {
  library(xgboost)
  library(caret)
})

results_list <- parLapply(cl, 1:nrow(param_grid), function(i) {
  train_xgb(as.numeric(param_grid[i, ]))
})

stopCluster(cl)

results_xgb <- bind_rows(results_list)
```
```{r}
#write.csv(results_xgb, "Data/liestal_xgb_results.csv", row.names = FALSE)
```

