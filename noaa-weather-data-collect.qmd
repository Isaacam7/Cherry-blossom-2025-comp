---
title: "Collecting noaa weather"
format: html
editor: visual
---

This quarto file is meant to document how I collect weather data to use for each of the sites.

```{r}
library(tidyverse)
library(httr2)
library(jsonlite)
library(R.utils)
library(glue)
library(rvest)
```

```{r}
token <- read.table("noaa-token.txt")$V1 #not in the github (my private token)
noaa_base_url <- 'https://www.ncei.noaa.gov/cdo-web/api/v2/'
noaa_base_req <- request(noaa_base_url) %>%
  req_headers(token = token) # add the API key to the request header
```

Hardest first.

# Liestal-Weideli

Due to the fact that historical data for liestal is not readily available I will try to supplement it by collecting data for regions in Switzerland near it and use a surrogate for liestal.

```{r}
get_station_list <- function(lat_lon,area_search){
  stations_list <- list()
  offset <- 1
  
  repeat {
    next_page <- noaa_base_req %>% 
      req_url_path_append('stations') %>% 
      req_url_query(
        extent = c(lat_lon[1] - area_search, 
                   lat_lon[2] - area_search,
                    lat_lon[1] + area_search,
                   lat_lon[2]	+ area_search) %>% 
          paste(collapse = ','),
        datasetid = 'GHCND',
        startdate = '2025-01-01',
        offset = offset,
        limit = 100
      ) %>%
      req_retry(max_tries = 10) %>%
      req_perform() |> 
      resp_body_json() 
    
    stations_list <- c(stations_list,
                     next_page$results)
    offset <- offset + next_page$metadata$resultset$limit
    
    if (offset > next_page$metadata$resultset$count) {
      break
    }
  }
  
  return(stations_list)
}
```

```{r}
swizz_stations_list <- get_station_list(c(47.4814,7.730519),0.25)

map(swizz_stations_list, as.data.frame) %>%
  do.call(what=rbind) %>%
  select(mindate,maxdate,name,id,datacoverage)
```

Two areas that seem to be relatively close with decent time span and good data coverage. Next I will pull all available data for each station. Rather than call using the api the data will be downloaded straight from the FTP (as the rate limit will likely break under thousands of calls at once)

```{r}
station_FTP_collect_process <- function(id){
  
  url <-glue("https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/{id}.csv.gz")
  
  dest.file <- glue("Data/{id}.csv.gz")
  download.file(url, dest.file, mode = "wb")
  gunzip(dest.file, overwrite = TRUE, remove = TRUE) 
  

  #now format data correctly
  
  dest.file <- gsub(".gz", "", dest.file)
  
  read_csv(dest.file,col_names = F) %>%
    select(1:4) %>%
    rename(STATION=X1,DATE=X2,ELEMENT=X3,
           VALUE=X4) %>%
    mutate(
      #convert temperature values to normal celcius (stored in tenths of degrees) 
      VALUE = ifelse(ELEMENT %in% c("TMAX", "TMIN", "TAVG"), VALUE / 10, VALUE),
      DATE = ymd(DATE) 
    ) %>%
    pivot_wider(names_from = ELEMENT, values_from = VALUE) %>%
    arrange(DATE) %>%
    write.csv(.,file=dest.file,row.names = FALSE)
  
    return(glue("{dest.file} processed"))
}
```

```{r}
map(swizz_stations_list, as.data.frame) %>%
  do.call(what=rbind) %>%
  select(id) %>%
  pull() %>%
  gsub("GHCND:", "", .) %>%
  as.list() %>%
  lapply(., station_FTP_collect_process)

```

Now I will collect the most recent Liestal weather data to compare

```{r}
get_weather_table <- function(url)
  read_html(url) %>% 
    html_nodes("div.monthly-calendar") %>% 
    html_text2() %>%
    str_replace("N/A", "N/A N/A") %>%
    str_remove_all("Â°|Hist. Avg. ") %>%
    str_split(" ", simplify = TRUE) %>%
    parse_number() %>%
    matrix(ncol = 3, 
           byrow = TRUE,
           dimnames = list(NULL, c("day", "tmax", "tmin"))) %>%
    as_tibble() %>%
    slice(which(day == 1)[1]:n()) %>%
  { 
      ones <- which(.$day == 1)
      if (length(ones) >= 2) {
        slice(., 1:(ones[2] - 1))
      } else {
        .  
      }
    }
```

```{r}
liestal_2024 <-
  tibble(
    base_url = "https://web.archive.org/web/20250226/https://www.accuweather.com/en/ch/liestal/311994/",
    month = month.name[1:12],
    year = 2024,
    url = str_c(base_url, tolower(month), "-weather/311994?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2024-01-01"), as.Date("2024-12-31"), 1),
            year = parse_number(format(date, "%Y")),
            tmax = (tmax - 32)* 5/9 ,
            tmin = (tmin - 32)* 5/9 ,
            temp = (tmax + tmin) / 2)  
```

Calculate average difference in temperature

```{r}
basel_2024 <- read_csv("Data/SZ000001940.csv") %>%
  filter(grepl("^2024", as.character(DATE)))

bale_2024 <- read_csv("Data/FRM00007299.csv") %>%
  filter(grepl("^2024", as.character(DATE)))
```

```{r}
mean(abs(liestal_2024$tmax - basel_2024$TMAX), na.rm = TRUE)

mean(abs(liestal_2024$tmax - bale_2024$TMAX), na.rm = TRUE)
```

```{r}
mean(abs(liestal_2024$tmin - basel_2024$TMIN), na.rm = TRUE)

mean(abs(liestal_2024$tmin - bale_2024$TMIN), na.rm = TRUE)
```

```{r}
mean(abs(liestal_2024$temp - basel_2024$TAVG), na.rm = TRUE)

mean(abs(liestal_2024$temp - bale_2024$TAVG), na.rm = TRUE)
```

Basel is generally closer in temperature, so I will continue with Basel

```{r}
(cor(liestal_2024$tmax, basel_2024$TMAX, use = "complete.obs"))
(cor(liestal_2024$tmin, basel_2024$TMIN, use = "complete.obs"))
(cor(liestal_2024$temp, basel_2024$TAVG, use = "complete.obs"))
```

Strong correlation, I will continue with basel as the surrogate.

```{r}
read_csv("Data/SZ000001940.csv") %>%
  select(-STATION) %>%
  mutate(
    
    TAVG = ifelse(is.na(TAVG) & !is.na(TMIN) & !is.na(TMAX), (TMIN + TMAX) / 2, TAVG),
    
    TMIN = ifelse(is.na(TMIN) & !is.na(TAVG) & !is.na(TMAX), (2 * TAVG - TMAX), TMIN),
    
    TMAX = ifelse(is.na(TMAX) & !is.na(TAVG) & !is.na(TMIN), (2 * TAVG - TMIN), TMAX)
  ) %>%
  write.csv(.,file="Data/liestal-weather-data.csv",row.names = F)
```

# Kyoto

```{r}
kyoto_stations_list <- get_station_list(c(35.0120,135.6761),0.25)

map(kyoto_stations_list, as.data.frame) %>%
  do.call(what=rbind) %>%
  select(mindate,maxdate,name,id,datacoverage)
```

Only 1 dataset so this shall be the weather data for kyoto

```{r}
map(kyoto_stations_list, as.data.frame) %>%
  do.call(what=rbind) %>%
  select(id) %>%
  pull() %>%
  gsub("GHCND:", "", .) %>%
  as.list() %>%
  lapply(., station_FTP_collect_process)
```

```{r}
read_csv("Data/JA000047759.csv") %>%
  select(-STATION) %>%
  mutate(
    
    TAVG = ifelse(is.na(TAVG) & !is.na(TMIN) & !is.na(TMAX), (TMIN + TMAX) / 2, TAVG),
    
    TMIN = ifelse(is.na(TMIN) & !is.na(TAVG) & !is.na(TMAX), (2 * TAVG - TMAX), TMIN),
    
    TMAX = ifelse(is.na(TMAX) & !is.na(TAVG) & !is.na(TMIN), (2 * TAVG - TMIN), TMAX)
  ) %>%
  write.csv(.,file="Data/kyoto-weather-data.csv",row.names = F)
```
